{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change dir in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dvc-cc-hide\n",
    "if os.getcwd().endswith('source'):\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow runs on the GPU:  True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Tensorflow runs on the GPU: ', tf.config.list_physical_devices('GPU'))\n",
    "print()\n",
    "parser = argparse.ArgumentParser();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training\n",
    "parser.add_argument('-lr', '--learning-rate', type=float, help='', default=0.0001)\n",
    "parser.add_argument('-b','--batch-size', type=int, help='', default=64)\n",
    "parser.add_argument('--num-of-epochs', type=int, help='', default=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model structure\n",
    "parser.add_argument('--activation-function', type=str, help='', default='relu')\n",
    "parser.add_argument('--kernel-width', type=int, help='', default=3)\n",
    "parser.add_argument('--average-kernels', type=int, help='', default=32)\n",
    "parser.add_argument('--num-of-conv-layers', type=int, help='', default=5)\n",
    "parser.add_argument('--kernel-increasing-factor', type=float, help='', default=1.2)\n",
    "parser.add_argument('--maxpool-after-n-layer', type=int, help='', default=0)\n",
    "parser.add_argument('--dropout-factor-after-conv', type=float, help='', default=0.1)\n",
    "parser.add_argument('--dropout-factor-after-maxp', type=float, help='', default=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dvc-cc-show\n",
    "args = parser.parse_args()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dvc-cc-hide\n",
    "args = parser.parse_args(\"--num-of-epochs 5 -b 16\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "padding = 'same'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2d = (args.kernel_width, args.kernel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 86, 86, 20)        560       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 86, 86, 20)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 86, 86, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 86, 86, 24)        4344      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 86, 86, 24)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 86, 86, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 86, 86, 29)        6293      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 86, 86, 29)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 86, 86, 29)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 86, 86, 35)        9170      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 86, 86, 35)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 86, 86, 35)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 86, 86, 42)        13272     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 86, 86, 42)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 86, 86, 42)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 86        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 33,725\n",
      "Trainable params: 33,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "for i in range(args.num_of_conv_layers):\n",
    "    kernels = args.average_kernels * (args.kernel_increasing_factor ** (i-(args.num_of_conv_layers/2.)))\n",
    "    kernels = int(kernels+0.5)\n",
    "    \n",
    "    if i == 0:\n",
    "        input_shape = list([96,96,3])\n",
    "        # use_cropping:\n",
    "        input_shape[0] -= 10\n",
    "        input_shape[1] -= 10 \n",
    "        \n",
    "        model.add(Conv2D(kernels, kernel2d, padding=padding,\n",
    "                 input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(Conv2D(kernels, kernel2d, padding=padding))\n",
    "    model.add(Activation(args.activation_function))\n",
    "    if args.maxpool_after_n_layer > 0 and (i+1) % args.maxpool_after_n_layer == 0:\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if args.dropout_factor_after_maxp > 0:\n",
    "            model.add(Dropout(args.dropout_factor_after_maxp))\n",
    "    elif args.dropout_factor_after_conv > 0:\n",
    "        model.add(Dropout(args.dropout_factor_after_conv))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "if args.dropout_factor_after_maxp > 0:\n",
    "    model.add(Dropout(args.dropout_factor_after_maxp))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_data_pcam(x_path,y_path,bz=args.batch_size):\n",
    "    x = h5py.File(x_path,'r', libver='latest',swmr=True)['x']\n",
    "    y = h5py.File(y_path,'r', libver='latest', swmr=True)['y']\n",
    "\n",
    "    datalen = len(x)\n",
    "    while True:\n",
    "        indizies = None\n",
    "        while indizies is None or len(indizies) == bz:\n",
    "            indizies = list(np.unique(sorted(np.random.randint(datalen,size=bz))))\n",
    "        \n",
    "        x_data = np.array(x[indizies])\n",
    "        \n",
    "        # normalize_input\n",
    "        x_data = x_data/256.0\n",
    "        \n",
    "        #use_cropping\n",
    "        r = np.random.randint(10)\n",
    "        r2 = np.random.randint(10)\n",
    "        x_data = x_data[:,r:-10+r,r2:-10+r2]\n",
    "        \n",
    "        # flip_input\n",
    "        if np.random.randint(2) == 1:\n",
    "            x_data = x_data[:,::-1]\n",
    "        if np.random.randint(2) == 1:\n",
    "            x_data = x_data[:,:,::-1]\n",
    "        \n",
    "        yield x_data, np.array([[1,0],[0,1]])[y[indizies][:,0,0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1108 19:31:48.377960 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:31:50.047948 139897357526848 callbacks.py:244] Method (on_train_batch_end) is slow compared to the batch update (0.122980). Check your callbacks.\n",
      "W1108 19:31:54.714812 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 8s - loss: 0.6949 - accuracy: 0.4867 - auc: 0.4899 - val_loss: 0.6965 - val_accuracy: 0.4500 - val_auc: 0.4500\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:32:00.757679 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 6s - loss: 0.6916 - accuracy: 0.5420 - auc: 0.5453 - val_loss: 0.6832 - val_accuracy: 0.6333 - val_auc: 0.6528\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:32:06.845517 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 6s - loss: 0.6644 - accuracy: 0.6207 - auc: 0.6596 - val_loss: 0.6439 - val_accuracy: 0.5667 - val_auc: 0.6747\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:32:12.821516 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 6s - loss: 0.6435 - accuracy: 0.6313 - auc: 0.6834 - val_loss: 0.6714 - val_accuracy: 0.5667 - val_auc: 0.6233\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:32:18.809404 139897357526848 training_generator.py:413] Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 6s - loss: 0.6449 - accuracy: 0.6300 - auc: 0.6773 - val_loss: 0.6220 - val_accuracy: 0.6833 - val_auc: 0.7253\n"
     ]
    }
   ],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard('tensorboard')\n",
    "history = model.fit_generator(next_data_pcam(\n",
    "                             'data/camelyonpatch_level_2_split_train_x.h5',\n",
    "                             'data2/camelyonpatch_level_2_split_train_y.h5'),\n",
    "                        validation_steps=10,\n",
    "                        steps_per_epoch=30,\n",
    "                        epochs=args.num_of_epochs,\n",
    "                        validation_data=next_data_pcam(\n",
    "                             'data3/camelyonpatch_level_2_split_valid_x.h5',\n",
    "                             'data4/camelyonpatch_level_2_split_valid_y.h5'),\n",
    "                        workers=1,\n",
    "                        verbose=2,\n",
    "                        use_multiprocessing=False,\n",
    "                        callbacks=[tensorboard])\n",
    "\n",
    "model.save_weights('tf_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputs'):\n",
    "    os.mkdir('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/all-history.json','w') as f:\n",
    "    json.dump(str(history.history),f)\n",
    "\n",
    "params = {}\n",
    "for p in history.history:\n",
    "    if p.find('loss') >= 0:\n",
    "        params[p] = np.min(history.history[p])\n",
    "    else:\n",
    "        params[p] = np.max(history.history[p])\n",
    "        \n",
    "with open('outputs/history-summary.json','w') as f:\n",
    "    json.dump(str(params),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
